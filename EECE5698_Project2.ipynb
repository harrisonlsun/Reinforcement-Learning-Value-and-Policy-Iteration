{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Reinforcement Learning  \n",
    "Author: Harrison Sun (sun.har@northeastern.edu)  \n",
    "GitHub: https://github.com/harrisonlsun/Reinforcement-Learning-Value-and-Policy-Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Maze Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import libraries\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "State_Matrix = \\\n",
    "    np.array([[np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN, np.NAN],\n",
    "              [np.NAN, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, np.NAN],\n",
    "              [np.NAN, 214, 215, 216, 217,   np.NAN, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, np.NAN],\n",
    "              [np.NAN, 197, 198, 199, 200,   np.NAN, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, np.NAN],\n",
    "              [np.NAN, 193, 194,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN, 195, 196, np.NAN],\n",
    "              [np.NAN, 176, 177,   np.NAN, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, np.NAN],\n",
    "              [np.NAN, 162, 163,   np.NAN, 164, 165,   np.NAN, 166, 167,   np.NAN, 168, 169, 170, 171, 172,   np.NAN, 173, 174, 175, np.NAN],\n",
    "              [np.NAN, 151, 152,   np.NAN, 153, 154,   np.NAN, 155, 156,   np.NAN, 157, 158,   np.NAN,   np.NAN,   np.NAN,   np.NAN, 159, 160, 161, np.NAN],\n",
    "              [np.NAN, 136, 137, 138, 139, 140,   np.NAN, 141, 142,   np.NAN, 143, 144, 145, 146, 147,   np.NAN, 148, 149, 150, np.NAN],\n",
    "              [np.NAN, 121, 122, 123, 124, 125,   np.NAN, 126, 127,   np.NAN, 128, 129, 130, 131, 132,   np.NAN, 133, 134, 135, np.NAN],\n",
    "              [np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN, 111,   np.NAN, 112, 113,   np.NAN,   np.NAN, 114, 115, 116, 117,   np.NAN, 118, 119, 120, np.NAN],\n",
    "              [np.NAN,  99, 100, 101, 102, 103,   np.NAN, 104, 105, 106,   np.NAN, 107, 108,   np.NAN, 109,   np.NAN,   np.NAN,   np.NAN, 110, np.NAN],\n",
    "              [np.NAN,  89,  90,   np.NAN,   np.NAN,   np.NAN,  np.NAN,   np.NAN,  91,  92,   np.NAN,  93,  94,   np.NAN,  95,  96,  97,   np.NAN,  98, np.NAN],\n",
    "              [np.NAN,  75,  76,  77,  78,  79,  80,   np.NAN,  81,  82,   np.NAN,  83,  84,   np.NAN,  85,  86,  87,   np.NAN,  88, np.NAN],\n",
    "              [np.NAN,  60,  61,  62,  63,  64,  65,   np.NAN,  66,  67,   np.NAN,  68,  69,   np.NAN,  70,  71,  72,  73,  74, np.NAN],\n",
    "              [np.NAN,  47,  48,  49,  50,  51,  52,   np.NAN,  53,  54,  55,  56,  57,   np.NAN,   np.NAN,   np.NAN,   np.NAN,  58,  59, np.NAN],\n",
    "              [np.NAN,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46, np.NAN],\n",
    "              [np.NAN,   np.NAN,   np.NAN,  19,  20,  21,  22,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,  23,  24,  25,  26,  27,  28, np.NAN],\n",
    "              [np.NAN,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,  16,  17,  18, np.NAN],\n",
    "              [np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN,   np.NAN, np.NAN]])\n",
    "        \n",
    "\n",
    "plt.subplots(figsize=(10,7.5))\n",
    "heatmap = sns.heatmap(State_Matrix, fmt=\".2f\", linewidths=0.25, linecolor='black',\n",
    "                      cbar= False, cmap= 'Purples')\n",
    "heatmap.set_facecolor('black') # Color for the NaN cells in the state matrix\n",
    "plt.title('Maze Problem')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coloring_blocks(heatmap, oil_states, bump_states, start_state, end_state):\n",
    "    # Adding red oil blocks\n",
    "    for i in range(len(oil_states)):\n",
    "        heatmap.add_patch(Rectangle((oil_states[i][1], oil_states[i][0]), 1, 1,\n",
    "                                    fill=True, facecolor='red', edgecolor='black', lw=0.25))\n",
    "    # Adding salmon bump blocks\n",
    "    for i in range(len(bump_states)):\n",
    "        heatmap.add_patch(Rectangle((bump_states[i][1], bump_states[i][0]), 1, 1,\n",
    "                                    fill=True, facecolor='lightsalmon', edgecolor='black', lw=0.25))\n",
    "    # Adding start block (Blue)\n",
    "    heatmap.add_patch(Rectangle((start_state[1], start_state[0]), 1, 1,\n",
    "                                fill=True, facecolor='lightblue', edgecolor='black', lw=0.25))\n",
    "\n",
    "    # Adding end block (Green)\n",
    "    heatmap.add_patch(Rectangle((end_state[1], end_state[0]), 1, 1,\n",
    "                                fill=True, facecolor='lightgreen', edgecolor='black', lw=0.25))\n",
    "\n",
    "plt.subplots(figsize=(10,7.5))    \n",
    "heatmap = sns.heatmap(State_Matrix, fmt=\".2f\", linewidths=0.25, linecolor='black',\n",
    "                      cbar= False, cmap= 'Purples')\n",
    "heatmap.set_facecolor('black') # Color for the NaN cells in the state matrix\n",
    "coloring_blocks(heatmap, oil_states=[(4,2),(5,6), (2,8), (2,16), (10,18), (15,10), (16,10), (18,7), (17,14), (17,17)], bump_states=[(5,1), (7,2), (1,11), (1,12), (2,1), (2,2), (2,3), (5,17), (6,17), (7,17), (8,17), (7,10), (7,11), (5,9), (12,11), (12,12), (15,17), (15,18), (16,7), (14,1), (14,2)], \\\n",
    "                start_state=(15,4),end_state=(3,13))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = +200      # Reward for reaching the end state\n",
    "oil = -5        # Reward for stepping on an oil block\n",
    "bump = -10      # Reward for stepping on a bump block\n",
    "action = -1     # Reward for taking an action (every move)\n",
    "goal_state = (3,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(s):\n",
    "    if s == (3, 13): # End state\n",
    "        return action + end\n",
    "    elif s in [(4,2),(5,6), (2,8), (2,16), (10,18), (15,10), (16,10), (18,7), (17,14), (17,17)]:\n",
    "        return action + oil\n",
    "    elif s in [(5,1), (7,2), (1,11), (1,12), (2,1), (2,2), (2,3), (5,17), (6,17), (7,17), (8,17), (7,10), (7,11), (5,9), (12,11), (12,12), (15,17), (15,18), (16,7), (14,1), (14,2)]:\n",
    "        return action + bump\n",
    "    else:\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy_path(policy, start_state, grid_shape):\n",
    "    direction_mapping = {0: 'up', 1: 'down', 2: 'left', 3: 'right'}\n",
    "    current_state = start_state\n",
    "    path = []\n",
    "    \n",
    "    # Generate the path based on the policy\n",
    "    for _ in range(np.prod(grid_shape)):  # Limit the path length to avoid infinite loops\n",
    "        action = policy[current_state]\n",
    "        direction = direction_mapping[action]\n",
    "        path.append((current_state, direction))\n",
    "        \n",
    "        if direction == 'right':\n",
    "            next_state = (current_state[0], current_state[1] + 1)\n",
    "        elif direction == 'left':\n",
    "            next_state = (current_state[0], current_state[1] - 1)\n",
    "        elif direction == 'up':\n",
    "            next_state = (current_state[0] - 1, current_state[1])\n",
    "        elif direction == 'down':\n",
    "            next_state = (current_state[0] + 1, current_state[1])\n",
    "        \n",
    "        if 0 <= next_state[0] < grid_shape[0] and 0 <= next_state[1] < grid_shape[1]:\n",
    "            current_state = next_state\n",
    "        else:\n",
    "            break  # Stop if we reach the edge of the grid\n",
    "    \n",
    "    # Heatmap generation\n",
    "    plt.subplots(figsize=(10,7.5))    \n",
    "    heatmap = sns.heatmap(State_Matrix, fmt=\".2f\", linewidths=0.25, linecolor='black',\n",
    "                        cbar= False, cmap= 'Purples')\n",
    "    heatmap.set_facecolor('black')  # Color for the NaN cells in the state matrix\n",
    "\n",
    "    # coloring_blocks function\n",
    "    coloring_blocks(heatmap, oil_states=[(4,2),(5,6), (2,8), (2,16), (10,18), (15,10), (16,10), (18,7), (17,14), (17,17)], \n",
    "                    bump_states=[(5,1), (7,2), (1,11), (1,12), (2,1), (2,2), (2,3), (5,17), (6,17), (7,17), (8,17), (7,10), (7,11), (5,9), (12,11), (12,12), (15,17), (15,18), (16,7), (14,1), (14,2)], \n",
    "                    start_state=(15,4), end_state=(3,13))\n",
    "\n",
    "    # Plotting the path with arrows\n",
    "    for state_cr, direction in path:\n",
    "        r, c = state_cr\n",
    "        if state_cr == goal_state:\n",
    "            break\n",
    "        if direction == 'right':\n",
    "            plt.arrow(c + 0.5, r + 0.5, 0.8, 0, width=0.04, color='black')\n",
    "        elif direction == 'left':\n",
    "            plt.arrow(c + 0.5, r + 0.5, -0.8, 0, width=0.04, color='black')\n",
    "        elif direction == 'up':\n",
    "            plt.arrow(c + 0.5, r + 0.5, 0, -0.8, width=0.04, color='black')\n",
    "        elif direction == 'down':\n",
    "            plt.arrow(c + 0.5, r + 0.5, 0, 0.8, width=0.04, color='black')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_values(State_Matrix, policy):\n",
    "    plt.subplots(figsize=(13,7.5))\n",
    "\n",
    "    # Create a 2D matrix of zeros with size of 20 x 20 \n",
    "    State_Matrix_heatmap = np.zeros((20,20), dtype = int) \n",
    "\n",
    "    for i in range(19):\n",
    "        for j in range(19):\n",
    "            State_Matrix_heatmap[i, j] = int(V[i][j])\n",
    "\n",
    "    heatmap = sns.heatmap(State_Matrix, fmt=\"d\", annot=State_Matrix_heatmap, linewidths=0.25, linecolor='black',\n",
    "                          cbar=False, cmap='Purples')\n",
    "\n",
    "    heatmap.set_facecolor('black') \n",
    "    coloring_blocks(heatmap, oil_states=[(4,2),(5,6), (2,8), (2,16), (10,18), (15,10), (16,10), (18,7), (17,14), (17,17)], bump_states=[(5,1), (7,2), (1,11), (1,12), (2,1), (2,2), (2,3), (5,17), (6,17), (7,17), (8,17), (7,10), (7,11), (5,9), (12,11), (12,12), (15,17), (15,18), (16,7), (14,1), (14,2)], \\\n",
    "                    start_state=(15,4),end_state=(3,13))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_policy(State_Matrix, Values):\n",
    "    plt.subplots(figsize=(10,7.5))\n",
    "    heatmap = sns.heatmap(State_Matrix, fmt=\".2f\", linewidths=0.25, linecolor='black',\n",
    "                          cbar=False, cmap='Purples')\n",
    "    heatmap.set_facecolor('black')  # Color for the NaN cells in the state matrix\n",
    "\n",
    "    # coloring_blocks function\n",
    "    coloring_blocks(heatmap, oil_states=[(4,2),(5,6), (2,8), (2,16), (10,18), (15,10), (16,10), (18,7), (17,14), (17,17)], bump_states=[(5,1), (7,2), (1,11), (1,12), (2,1), (2,2), (2,3), (5,17), (6,17), (7,17), (8,17), (7,10), (7,11), (5,9), (12,11), (12,12), (15,17), (15,18), (16,7), (14,1), (14,2)], \\\n",
    "                    start_state=(15,4),end_state=(3,13))\n",
    "\n",
    "    # Plotting the arrows based on the optimal policy\n",
    "    for i in range(State_Matrix.shape[0]):\n",
    "        for j in range(State_Matrix.shape[1]):\n",
    "            if np.isnan(State_Matrix[i, j]):\n",
    "                continue\n",
    "            if (i, j) == goal_state:\n",
    "                continue\n",
    "            max_adjacent_value = -np.inf\n",
    "            max_adjacent_direction = None\n",
    "            for direction in ['up', 'down', 'left', 'right']:\n",
    "                if direction == 'up':\n",
    "                    adjacent_value = Values[i-1, j] if i > 0 else Values[i, j]\n",
    "                elif direction == 'down':\n",
    "                    adjacent_value = Values[i+1, j] if i < State_Matrix.shape[0]-1 else Values[i, j]\n",
    "                elif direction == 'left':\n",
    "                    adjacent_value = Values[i, j-1] if j > 0 else Values[i, j]\n",
    "                elif direction == 'right':\n",
    "                    adjacent_value = Values[i, j+1] if j < State_Matrix.shape[1]-1 else Values[i, j]\n",
    "                if adjacent_value > max_adjacent_value:\n",
    "                    max_adjacent_value = adjacent_value\n",
    "                    max_adjacent_direction = direction\n",
    "\n",
    "            if max_adjacent_direction == 'up':\n",
    "                plt.arrow(j + 0.5, i + 0.5, 0, -0.4, width=0.04, color='black')\n",
    "            elif max_adjacent_direction == 'down':\n",
    "                plt.arrow(j + 0.5, i + 0.5, 0, 0.4, width=0.04, color='black')\n",
    "            elif max_adjacent_direction == 'left':\n",
    "                plt.arrow(j + 0.5, i + 0.5, -0.4, 0, width=0.04, color='black')\n",
    "            elif max_adjacent_direction == 'right':\n",
    "                plt.arrow(j + 0.5, i + 0.5, 0.4, 0, width=0.04, color='black')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: This portion uses vector-form Policy Iteration to find optimal policy and optimal state values.\n",
    "a. $p=0.02, \\gamma = 0.95, \\theta = 0.01$ (Base Scenario)  \n",
    "b. $p=0.5, \\gamma = 0.95, \\theta = 0.01$ (Large Stochasticity Scenario)  \n",
    "c. $p=0.02, \\gamma = 0.55, \\theta = 0.01$ (Small Discount Factor Scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_form_policy_iteration(State_Matrix, p, gamma, theta):\n",
    "    V = np.zeros(State_Matrix.shape)\n",
    "    policy = np.random.randint(0, 4, State_Matrix.shape)\n",
    "    actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Up, Down, Left, Right\n",
    "\n",
    "    while True:\n",
    "        # Policy Evaluation\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for i in range(State_Matrix.shape[0]):\n",
    "                for j in range(State_Matrix.shape[1]):\n",
    "                    if not np.isnan(State_Matrix[i, j]):\n",
    "                        v = V[i, j]\n",
    "                        bellman_sum = 0\n",
    "                        for action, (a, b) in enumerate(actions):\n",
    "                            next_i, next_j = i + a, j + b\n",
    "                            if 0 <= next_i < State_Matrix.shape[0] and 0 <= next_j < State_Matrix.shape[1] and not np.isnan(State_Matrix[next_i, next_j]):\n",
    "                                prob = p if policy[i, j] == action else (1 - p) / 3\n",
    "                                bellman_sum += prob * (reward((next_i, next_j)) + gamma * V[next_i, next_j])\n",
    "                            else:\n",
    "                                bellman_sum += (p if policy[i, j] == action else (1 - p) / 3) * (reward((i, j)) + gamma * V[i, j])\n",
    "                        V[i, j] = bellman_sum\n",
    "                        delta = max(delta, np.abs(v - V[i, j]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        # Policy Improvement\n",
    "        policy_stable = True\n",
    "        for i in range(State_Matrix.shape[0]):\n",
    "            for j in range(State_Matrix.shape[1]):\n",
    "                if not np.isnan(State_Matrix[i, j]):\n",
    "                    old_action = policy[i, j]\n",
    "                    action_values = []\n",
    "                    for action, (a, b) in enumerate(actions):\n",
    "                        next_i, next_j = i + a, j + b\n",
    "                        if 0 <= next_i < State_Matrix.shape[0] and 0 <= next_j < State_Matrix.shape[1] and not np.isnan(State_Matrix[next_i, next_j]):\n",
    "                            action_value = p * (reward((next_i, next_j)) + gamma * V[next_i, next_j]) + (1 - p) / 3 * sum(reward((next_i if 0 <= next_i < State_Matrix.shape[0] and 0 <= next_j < State_Matrix.shape[1] else i, next_j if 0 <= next_i < State_Matrix.shape[0] and 0 <= next_j < State_Matrix.shape[1] else j)) + gamma * V[next_i if 0 <= next_i < State_Matrix.shape[0] and 0 <= next_j < State_Matrix.shape[1] else i, next_j if 0 <= next_i < State_Matrix.shape[0] and 0 <= next_j < State_Matrix.shape[1] else j] for other_action in actions if other_action != (a, b))\n",
    "                        else:\n",
    "                            action_value = reward((i, j)) + gamma * V[i, j]\n",
    "                        action_values.append(action_value)\n",
    "                    policy[i, j] = np.argmax(action_values)\n",
    "                    if old_action != policy[i, j]:\n",
    "                        policy_stable = False\n",
    "        if policy_stable:\n",
    "            return policy, V\n",
    "        else:\n",
    "            continue # Not needed, but makes the code more readable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.a. Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Part A\n",
    "policy, V = vector_form_policy_iteration(State_Matrix, 1-0.02, 0.95, 0.01) \n",
    "print(\"Values\")\n",
    "draw_values(State_Matrix, policy)\n",
    "print(\"Optimal Policy\")\n",
    "draw_policy(State_Matrix, V)\n",
    "print(\"Optimal Path\")\n",
    "plot_policy_path(policy, (15, 4), State_Matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.b. Outputs (Large Stochasticity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Part B\n",
    "policy, V = vector_form_policy_iteration(State_Matrix, 1-0.5, 0.95, 0.01) \n",
    "print(\"Values\")\n",
    "draw_values(State_Matrix, policy)\n",
    "print(\"Optimal Policy\")\n",
    "draw_policy(State_Matrix, V)\n",
    "print(\"Optimal Path\")\n",
    "plot_policy_path(policy, (15, 4), State_Matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.c. Outputs (Small Discount Factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Part C\n",
    "policy, V = vector_form_policy_iteration(State_Matrix, 1-0.02, 0.55, 0.01) \n",
    "print(\"Values\")\n",
    "draw_values(State_Matrix, policy)\n",
    "print(\"Optimal Policy\")\n",
    "draw_policy(State_Matrix, V)\n",
    "print(\"Optimal Path\")\n",
    "plot_policy_path(policy, (15, 4), State_Matrix.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
